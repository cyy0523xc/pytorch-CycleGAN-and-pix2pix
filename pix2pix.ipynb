{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wNjDKdQy35h"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TRm-USlsHgEV"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt3igws3eiVp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z1EySlOXwwoa"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt  -i https://pypi.tuna.tsinghua.edu.cn/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8daqlgVhw29P"
   },
   "source": [
    "# Datasets\n",
    "\n",
    "Download one of the official datasets with:\n",
    "\n",
    "-   `bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]`\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrdOettJxaCc"
   },
   "outputs": [],
   "source": [
    "!bash ./datasets/download_pix2pix_dataset.sh facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdUz4116xhpm"
   },
   "source": [
    "# Pretrained models\n",
    "\n",
    "Download one of the official pretrained models with:\n",
    "\n",
    "-   `bash ./scripts/download_pix2pix_model.sh [edges2shoes, sat2map, map2sat, facades_label2photo, and day2night]`\n",
    "\n",
    "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GC2DEP4M0OsS"
   },
   "outputs": [],
   "source": [
    "!bash ./scripts/download_pix2pix_model.sh facades_label2photo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 启动Visdom\n",
    "\n",
    "http://192.168.80.242:20941/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yFw1kDQBx3LN"
   },
   "source": [
    "# Training\n",
    "\n",
    "-   `python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA`\n",
    "\n",
    "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. Add `--direction BtoA` if you want to train a model to transfrom from class B to A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sp7TCT2x9dB"
   },
   "outputs": [],
   "source": [
    "# --display_id -1\n",
    "# --gpu_ids 0,1\n",
    "# --display_server http://192.168.80.242 --display_port 20941\n",
    "# --save_latest_freq 20\n",
    "!python3 train.py --dataroot ./datasets/doc-clean --name doc_clean_pix2pix --model pix2pix --direction AtoB --display_server http://192.168.80.242 --display_port 20941 \\\n",
    "    --crop_size 512 --load_size 1000 --num_threads 4 --gpu_ids 0,1 --batch_size 12 --no_flip --netD pixel --netG unet_256 \\\n",
    "    --lr_policy linear --load_size_range 0.15 --loss_fun l1l2 --lambda_L1 50 --lambda_L2 16 --lambda_L2_w 0.25 \\\n",
    "    --brightness 0.15 --contrast 0.15 --saturation 0.15 --hue 0.1 \\\n",
    "    --output_nc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UkcaFZiyASl"
   },
   "source": [
    "# Testing\n",
    "\n",
    "-   `python test.py --dataroot ./datasets/facades --direction BtoA --model pix2pix --name facades_pix2pix`\n",
    "\n",
    "Change the `--dataroot`, `--name`, and `--direction` to be consistent with your trained model's configuration and how you want to transform images.\n",
    "\n",
    "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
    "> Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.\n",
    "\n",
    "> If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).\n",
    "\n",
    "> See a list of currently available models at ./scripts/download_pix2pix_model.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mey7o6j-0368"
   },
   "outputs": [],
   "source": [
    "!ls checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCsKkEq0yGh0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.aligned_dataset\n",
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 16                            \t[default: 1]\n",
      "               brightness: 0.15                          \t[default: 0]\n",
      "          checkpoints_dir: ./checkpoints                 \n",
      "                 contrast: 0.15                          \t[default: 0]\n",
      "                crop_size: 512                           \t[default: 256]\n",
      "                 dataroot: ./datasets/doc-clean          \t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                      hue: 0.1                           \t[default: 0]\n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                lambda_L1: 50.0                          \t[default: 100.0]\n",
      "                lambda_L2: 16.0                          \n",
      "              lambda_L2_w: 0.25                          \t[default: 1]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 1000                          \t[default: 286]\n",
      "          load_size_range: 0.0                           \n",
      "                 loss_fun: l1l2                          \t[default: l2]\n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: doc_clean_pix2pix             \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: pixel                         \t[default: basic]\n",
      "                     netG: unet_256                      \n",
      "                      ngf: 64                            \n",
      "               no_convert: False                         \n",
      "               no_dropout: False                         \n",
      "                  no_flip: True                          \t[default: False]\n",
      "                     norm: batch                         \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 1                             \t[default: 3]\n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: ./results/                    \n",
      "               saturation: 0.15                          \t[default: 0]\n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "              test_epochs: 10                            \t[default: 1]\n",
      "                  verbose: False                         \n",
      "----------------- End -------------------\n",
      "data.aligned_dataset\n",
      "dataset [AlignedDataset] was created\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from ./checkpoints/doc_clean_pix2pix/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 54.410 M\n",
      "-----------------------------------------------\n",
      "creating web directory ./results/doc_clean_pix2pix/test_latest\n",
      "**************************************** 0 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.204277038574219\n",
      "Loss G_L1: 1.9035\n",
      "**************************************** 1 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.016756057739258\n",
      "Loss G_L1: 1.7349\n",
      "**************************************** 2 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.0886876583099365\n",
      "Loss G_L1: 1.9178\n",
      "**************************************** 3 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.150485277175903\n",
      "Loss G_L1: 1.9818\n",
      "**************************************** 4 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.05199933052063\n",
      "Loss G_L1: 2.2266\n",
      "**************************************** 5 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.065194606781006\n",
      "Loss G_L1: 1.7900\n",
      "**************************************** 6 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.192005157470703\n",
      "Loss G_L1: 1.9314\n",
      "**************************************** 7 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.10333514213562\n",
      "Loss G_L1: 1.7864\n",
      "**************************************** 8 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.148136854171753\n",
      "Loss G_L1: 1.7426\n",
      "**************************************** 9 ****************************************\n",
      "processing (0000)-th image... ['./datasets/doc-clean/test/i000018.png']\n",
      "processing (0005)-th image... ['./datasets/doc-clean/test/i000027.png']\n",
      "processing (0010)-th image... ['./datasets/doc-clean/test/i000055.png']\n",
      "processing (0015)-th image... ['./datasets/doc-clean/test/i000108.png']\n",
      "processing (0020)-th image... ['./datasets/doc-clean/test/i000132.png']\n",
      "processing (0025)-th image... ['./datasets/doc-clean/test/i000170.png']\n",
      "processing (0030)-th image... ['./datasets/doc-clean/test/i000206.png']\n",
      "processing (0035)-th image... ['./datasets/doc-clean/test/i000236.png']\n",
      "Time:  6.172861576080322\n",
      "Loss G_L1: 1.8680\n",
      "**************************************** Total Loss:  ****************************************\n",
      "    G_L1: 1.8883\n"
     ]
    }
   ],
   "source": [
    "!rm -rf results\n",
    "!python3 test.py --dataroot ./datasets/doc-clean --direction AtoB --model pix2pix --name doc_clean_pix2pix \\\n",
    "    --crop_size 512 --load_size 1000 --num_threads 4 --gpu_ids 0 --batch_size 16 --no_flip --netD pixel --netG unet_256 \\\n",
    "    --test_epochs 10 --loss_fun l1l2 --lambda_L1 50 --lambda_L2 16  --lambda_L2_w 0.25 \\\n",
    "    --brightness 0.15 --contrast 0.15 --saturation 0.15 --hue 0.1 \\\n",
    "    --output_nc 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf results\n",
    "!python3 predict.py --dataroot ./datasets/doc-clean --model pix2pix --name doc_clean_pix2pix \\\n",
    "    --crop_size 512 --load_size 1000 --num_threads 4 --gpu_ids 0 --batch_size 16 --no_flip --netD pixel --netG unet_256 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzSKIPUByfiN"
   },
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mgg8raPyizq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_fake_B.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G3oVH9DyqLQ"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_A.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErK5OC1j1LH4"
   },
   "outputs": [],
   "source": [
    "img = plt.imread('./results/facades_label2photo_pretrained/test_latest/images/100_real_B.png')\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pix2pix",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
